{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Project Team 8\n",
        "Aaliyah Brown\n",
        "Ashley Witte\n",
        "Prateek Verma\n",
        "Yuri Chen\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fqol9-DFPF85"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVI7_pCM9IpL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"yelp-dataset/yelp-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "WEArwE7U9Smp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Path to dataset files: /root/.cache/kagglehub/datasets/yelp-dataset/yelp-dataset/versions/4"
      ],
      "metadata": {
        "id": "b9-Y71v_9U_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_filepath = \"/root/.cache/kagglehub/datasets/yelp-dataset/yelp-dataset/versions/4/yelp_academic_dataset_review.json\"\n",
        "checkin_filepath = \"/root/.cache/kagglehub/datasets/yelp-dataset/yelp-dataset/versions/4/yelp_academic_dataset_checkin.json\"\n",
        "business_filepath = \"/root/.cache/kagglehub/datasets/yelp-dataset/yelp-dataset/versions/4/yelp_academic_dataset_business.json\"\n",
        "tip_filepath = \"/root/.cache/kagglehub/datasets/yelp-dataset/yelp-dataset/versions/4/yelp_academic_dataset_tip.json\"\n",
        "user_filepath = \"/root/.cache/kagglehub/datasets/yelp-dataset/yelp-dataset/versions/4/yelp_academic_dataset_user.json\""
      ],
      "metadata": {
        "id": "F2Pq3ACQ9cZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_chunks_iterator = pd.read_json(review_filepath, lines = True, chunksize = 100000)"
      ],
      "metadata": {
        "id": "S3mofrf59gs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# looking at the first 3 chunks to get a general intuition behind the full review data\n",
        "try:\n",
        "    chuck_1 = next(review_chunks_iterator)\n",
        "    chuck_2 = next(review_chunks_iterator)\n",
        "    chuck_3 = next(review_chunks_iterator)\n",
        "\n",
        "    chuck_1.info()\n",
        "    chuck_2.info()\n",
        "    chuck_3.info()\n",
        "except StopIteration:\n",
        "    print(\"No more chunks to process.\")"
      ],
      "metadata": {
        "id": "k8JRm5N69i7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "<class 'pandas.core.frame.DataFrame'>\n",
        "RangeIndex: 100000 entries, 0 to 99999\n",
        "Data columns (total 9 columns):\n",
        " #   Column       Non-Null Count   Dtype\n",
        "---  ------       --------------   -----\n",
        " 0   review_id    100000 non-null  object\n",
        " 1   user_id      100000 non-null  object\n",
        " 2   business_id  100000 non-null  object\n",
        " 3   stars        100000 non-null  int64\n",
        " 4   useful       100000 non-null  int64\n",
        " 5   funny        100000 non-null  int64\n",
        " 6   cool         100000 non-null  int64\n",
        " 7   text         100000 non-null  object\n",
        " 8   date         100000 non-null  datetime64[ns]\n",
        "dtypes: datetime64[ns](1), int64(4), object(4)\n",
        "memory usage: 6.9+ MB\n",
        "<class 'pandas.core.frame.DataFrame'>\n",
        "RangeIndex: 100000 entries, 100000 to 199999\n",
        "Data columns (total 9 columns):\n",
        " #   Column       Non-Null Count   Dtype\n",
        "---  ------       --------------   -----\n",
        " 0   review_id    100000 non-null  object\n",
        " 1   user_id      100000 non-null  object\n",
        " 2   business_id  100000 non-null  object\n",
        " 3   stars        100000 non-null  int64\n",
        " 4   useful       100000 non-null  int64\n",
        " 5   funny        100000 non-null  int64\n",
        " 6   cool         100000 non-null  int64\n",
        " 7   text         100000 non-null  object\n",
        " 8   date         100000 non-null  datetime64[ns]\n",
        "dtypes: datetime64[ns](1), int64(4), object(4)\n",
        "memory usage: 6.9+ MB\n",
        "<class 'pandas.core.frame.DataFrame'>\n",
        "RangeIndex: 100000 entries, 200000 to 299999\n",
        "Data columns (total 9 columns):\n",
        " #   Column       Non-Null Count   Dtype\n",
        "---  ------       --------------   -----\n",
        " 0   review_id    100000 non-null  object\n",
        " 1   user_id      100000 non-null  object\n",
        " 2   business_id  100000 non-null  object\n",
        " 3   stars        100000 non-null  int64\n",
        " 4   useful       100000 non-null  int64\n",
        " 5   funny        100000 non-null  int64\n",
        " 6   cool         100000 non-null  int64\n",
        " 7   text         100000 non-null  object\n",
        " 8   date         100000 non-null  datetime64[ns]\n",
        "dtypes: datetime64[ns](1), int64(4), object(4)\n",
        "memory usage: 6.9+ MB"
      ],
      "metadata": {
        "id": "rlB9u_zo9x1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"chuck 1: {chuck_1.isna().sum()}\")\n",
        "print(f\"chuck 2: {chuck_2.isna().sum()}\")\n",
        "print(f\"chuck 3: {chuck_3.isna().sum()}\")\n"
      ],
      "metadata": {
        "id": "3NsY1Cwr9prg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chuck 1: review_id      0\n",
        "user_id        0\n",
        "business_id    0\n",
        "stars          0\n",
        "useful         0\n",
        "funny          0\n",
        "cool           0\n",
        "text           0\n",
        "date           0\n",
        "dtype: int64\n",
        "chuck 2: review_id      0\n",
        "user_id        0\n",
        "business_id    0\n",
        "stars          0\n",
        "useful         0\n",
        "funny          0\n",
        "cool           0\n",
        "text           0\n",
        "date           0\n",
        "dtype: int64\n",
        "chuck 3: review_id      0\n",
        "user_id        0\n",
        "business_id    0\n",
        "stars          0\n",
        "useful         0\n",
        "funny          0\n",
        "cool           0\n",
        "text           0\n",
        "date           0\n",
        "dtype: int64"
      ],
      "metadata": {
        "id": "kmP57-w09sl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chuck_1['cool'].value_counts()"
      ],
      "metadata": {
        "id": "EMEgjlDp9vFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "count\n",
        "cool\n",
        "0\t79984\n",
        "1\t13394\n",
        "2\t3675\n",
        "3\t1337\n",
        "4\t635\n",
        "5\t351\n",
        "6\t191\n",
        "7\t122\n",
        "8\t80\n",
        "9\t68\n",
        "10\t42\n",
        "11\t30\n",
        "12\t18\n",
        "13\t11\n",
        "14\t11\n",
        "15\t8\n",
        "17\t7\n",
        "22\t5\n",
        "16\t5\n",
        "18\t4\n",
        "29\t2\n",
        "19\t2\n",
        "32\t2\n",
        "23\t2\n",
        "21\t2\n",
        "27\t2\n",
        "44\t1\n",
        "26\t1\n",
        "49\t1\n",
        "25\t1\n",
        "33\t1\n",
        "31\t1\n",
        "24\t1\n",
        "41\t1\n",
        "42\t1\n",
        "20\t1\n",
        "\n",
        "dtype: int64"
      ],
      "metadata": {
        "id": "4KvXNUEe93iT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Example 'text' values from chunk 1\")\n",
        "print(chuck_1['text'].head())"
      ],
      "metadata": {
        "id": "UOGpG6tR950h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Example 'text' values from chunk 1\n",
        "0    If you decide to eat here, just be aware it is...\n",
        "1    I've taken a lot of spin classes over the year...\n",
        "2    Family diner. Had the buffet. Eclectic assortm...\n",
        "3    Wow!  Yummy, different,  delicious.   Our favo...\n",
        "4    Cute interior and owner (?) gave us tour of up...\n",
        "Name: text, dtype: object"
      ],
      "metadata": {
        "id": "oL2IH0-297-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Example 'text' values from chunk 2\")\n",
        "print(chuck_2['text'].head())"
      ],
      "metadata": {
        "id": "-Jb597ig9-Dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Example 'text' values from chunk 2\n",
        "100000    Why 3 stars?\\n\\nI love Target, but here is jus...\n",
        "100001    The Local Taco is the first joint I've found i...\n",
        "100002    Top notch! He will cater our wedding and  Fran...\n",
        "100003    Best place to get a vietnamese sandwich and bo...\n",
        "100004    This is more like a 3 1/2 star rating. The foo...\n",
        "Name: text, dtype: object"
      ],
      "metadata": {
        "id": "NGswKqu9-AZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Example 'text' values from chunk 3\")\n",
        "print(chuck_3['text'].head())"
      ],
      "metadata": {
        "id": "wdnNiE_U-CGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Example 'text' values from chunk 3\n",
        "200000    Great craft beers, great fresh food.  Small me...\n",
        "200001    Establishments like The Libertine remind me th...\n",
        "200002    A sub is a sub is a sub, but not when it's Eeg...\n",
        "200003    I went for my birthday in oct. I had a large g...\n",
        "200004    The food here is soooooooo good! I have walked...\n",
        "Name: text, dtype: object"
      ],
      "metadata": {
        "id": "w6E_6Bbw-D9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Example 'date' values from chunks 1-3\")\n",
        "print(chuck_1['date'].head())\n",
        "print(chuck_2['date'].head())\n",
        "print(chuck_3['date'].head())"
      ],
      "metadata": {
        "id": "zBLL2V53-Fwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Example 'date' values from chunks 1-3\n",
        "0   2018-07-07 22:09:11\n",
        "1   2012-01-03 15:28:18\n",
        "2   2014-02-05 20:30:30\n",
        "3   2015-01-04 00:01:03\n",
        "4   2017-01-14 20:54:15\n",
        "Name: date, dtype: datetime64[ns]\n",
        "100000   2018-05-02 12:31:05\n",
        "100001   2011-03-29 04:44:08\n",
        "100002   2014-10-02 11:32:33\n",
        "100003   2015-04-20 14:30:49\n",
        "100004   2012-07-21 23:02:43\n",
        "Name: date, dtype: datetime64[ns]\n",
        "200000   2015-04-12 00:20:02\n",
        "200001   2013-05-23 18:27:42\n",
        "200002   2012-03-15 18:26:47\n",
        "200003   2017-01-11 23:01:18\n",
        "200004   2017-06-25 03:32:25\n",
        "Name: date, dtype: datetime64[ns]"
      ],
      "metadata": {
        "id": "D5mUxSyh-Hvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONNECT + UPLOAD TO AZURE"
      ],
      "metadata": {
        "id": "38F8oeh_-J3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt list --installed | grep odbc"
      ],
      "metadata": {
        "id": "AHJRukpe-naS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e925023c-0eba-44b1-8147-cbbf39d0e2dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "libodbc2/jammy-updates,jammy-security,now 2.3.9-5ubuntu0.1 amd64 [installed,automatic]\n",
            "libodbccr2/jammy-updates,jammy-security,now 2.3.9-5ubuntu0.1 amd64 [installed,automatic]\n",
            "libodbcinst2/jammy-updates,jammy-security,now 2.3.9-5ubuntu0.1 amd64 [installed,automatic]\n",
            "unixodbc-common/jammy-updates,jammy-security,now 2.3.9-5ubuntu0.1 all [installed,automatic]\n",
            "unixodbc-dev/jammy-updates,jammy-security,now 2.3.9-5ubuntu0.1 amd64 [installed,automatic]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
        "\n",
        "libodbc2/jammy-updates,jammy-security,now 2.3.9-5ubuntu0.1 amd64 [installed,automatic]\n",
        "libodbccr2/jammy-updates,jammy-security,now 2.3.9-5ubuntu0.1 amd64 [installed,automatic]\n",
        "libodbcinst2/jammy-updates,jammy-security,now 2.3.9-5ubuntu0.1 amd64 [installed,automatic]\n",
        "unixodbc-common/jammy-updates,jammy-security,now 2.3.9-5ubuntu0.1 all [installed,automatic]\n",
        "unixodbc-dev/jammy-updates,jammy-security,now 2.3.9-5ubuntu0.1 amd64 [installed,automatic]"
      ],
      "metadata": {
        "id": "tHXTGJW5-oG0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "bccae54d-0558-443d-81d1-77a34e3b3469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid decimal literal (ipython-input-2493853731.py, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2493853731.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    libodbc2/jammy-updates,jammy-security,now 2.3.9-5ubuntu0.1 amd64 [installed,automatic]\u001b[0m\n\u001b[0m                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas sqlalchemy pyodbc"
      ],
      "metadata": {
        "id": "a1W2yUfQ-qOx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2848c1c-5a93-451d-de11-e211f7d0183c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.12/dist-packages (2.0.44)\n",
            "Collecting pyodbc\n",
            "  Downloading pyodbc-5.3.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy) (3.2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy) (4.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pyodbc-5.3.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (340 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m340.3/340.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyodbc\n",
            "Successfully installed pyodbc-5.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
        "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.12/dist-packages (2.0.44)\n",
        "Collecting pyodbc\n",
        "  Downloading pyodbc-5.3.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
        "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
        "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
        "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
        "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
        "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy) (3.2.4)\n",
        "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy) (4.15.0)\n",
        "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
        "Downloading pyodbc-5.3.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (340 kB)\n",
        "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 340.3/340.3 kB 7.0 MB/s eta 0:00:00\n",
        "Installing collected packages: pyodbc\n",
        "Successfully installed pyodbc-5.3.0"
      ],
      "metadata": {
        "id": "oPUVsxG7-sxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import create_engine\n",
        "from urllib.parse import quote_plus"
      ],
      "metadata": {
        "id": "FBlYk18h-vL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Microsoft ODBC Driver for SQL Server\n",
        "!curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -\n",
        "!curl https://packages.microsoft.com/config/ubuntu/$(lsb_release -rs)/prod.list > /etc/apt/sources.list.d/mssql-release.list\n",
        "!apt-get update\n",
        "!ACCEPT_EULA=Y apt-get install -y msodbcsql18\n",
        "# Optional: Install the command line tools\n",
        "# !ACCEPT_EULA=Y apt-get install -y mssql-tools18\n",
        "# !echo 'export PATH=\"$PATH:/opt/mssql-tools18/bin\"' >> ~/.bashrc\n",
        "# !source ~/.bashrc"
      ],
      "metadata": {
        "id": "JngIjtOU-w44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae16cb72-53bb-4427-a7d8-22c127548457"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "100   975  100   975    0     0   3566      0 --:--:-- --:--:-- --:--:--  3558\n",
            "OK\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100    89  100    89    0     0    539      0 --:--:-- --:--:-- --:--:--   539\n",
            "Hit:1 https://cli.github.com/packages stable InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:4 https://packages.microsoft.com/ubuntu/22.04/prod jammy InRelease [3,632 B]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:7 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [83.2 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://packages.microsoft.com/ubuntu/22.04/prod jammy/main all Packages [1,331 B]\n",
            "Get:13 https://packages.microsoft.com/ubuntu/22.04/prod jammy/main armhf Packages [21.8 kB]\n",
            "Get:14 https://packages.microsoft.com/ubuntu/22.04/prod jammy/main arm64 Packages [94.9 kB]\n",
            "Get:15 https://packages.microsoft.com/ubuntu/22.04/prod jammy/main amd64 Packages [268 kB]\n",
            "Hit:16 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,822 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,411 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,855 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,289 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,594 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,161 kB]\n",
            "Get:24 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,520 kB]\n",
            "Get:25 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,963 kB]\n",
            "Fetched 35.5 MB in 4s (9,444 kB/s)\n",
            "Reading package lists... Done\n",
            "W: https://packages.microsoft.com/ubuntu/22.04/prod/dists/jammy/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  odbcinst unixodbc\n",
            "The following NEW packages will be installed:\n",
            "  msodbcsql18 odbcinst unixodbc\n",
            "0 upgraded, 3 newly installed, 0 to remove and 47 not upgraded.\n",
            "Need to get 791 kB of archives.\n",
            "After this operation, 164 kB of additional disk space will be used.\n",
            "Get:1 https://packages.microsoft.com/ubuntu/22.04/prod jammy/main amd64 msodbcsql18 amd64 18.5.1.1-1 [755 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 odbcinst amd64 2.3.9-5ubuntu0.1 [9,930 B]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 unixodbc amd64 2.3.9-5ubuntu0.1 [26.7 kB]\n",
            "Fetched 791 kB in 0s (1,654 kB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package odbcinst.\n",
            "(Reading database ... 125080 files and directories currently installed.)\n",
            "Preparing to unpack .../odbcinst_2.3.9-5ubuntu0.1_amd64.deb ...\n",
            "Unpacking odbcinst (2.3.9-5ubuntu0.1) ...\n",
            "Selecting previously unselected package unixodbc.\n",
            "Preparing to unpack .../unixodbc_2.3.9-5ubuntu0.1_amd64.deb ...\n",
            "Unpacking unixodbc (2.3.9-5ubuntu0.1) ...\n",
            "Selecting previously unselected package msodbcsql18.\n",
            "Preparing to unpack .../msodbcsql18_18.5.1.1-1_amd64.deb ...\n",
            "Unpacking msodbcsql18 (18.5.1.1-1) ...\n",
            "Setting up odbcinst (2.3.9-5ubuntu0.1) ...\n",
            "Setting up unixodbc (2.3.9-5ubuntu0.1) ...\n",
            "Setting up msodbcsql18 (18.5.1.1-1) ...\n",
            "odbcinst: Driver installed. Usage count increased to 1. \n",
            "    Target directory is /etc\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
        "                                 Dload  Upload   Total   Spent    Left  Speed\n",
        "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
        "100   975  100   975    0     0   6737      0 --:--:-- --:--:-- --:--:--  6770\n",
        "OK\n",
        "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
        "                                 Dload  Upload   Total   Spent    Left  Speed\n",
        "100    89  100    89    0     0    666      0 --:--:-- --:--:-- --:--:--   669\n",
        "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
        "Hit:2 https://cli.github.com/packages stable InRelease\n",
        "Get:3 https://packages.microsoft.com/ubuntu/22.04/prod jammy InRelease [3,632 B]\n",
        "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
        "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
        "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [83.2 kB]\n",
        "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
        "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
        "Hit:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
        "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
        "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
        "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
        "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
        "Get:14 https://packages.microsoft.com/ubuntu/22.04/prod jammy/main all Packages [1,331 B]\n",
        "Get:15 https://packages.microsoft.com/ubuntu/22.04/prod jammy/main arm64 Packages [94.9 kB]\n",
        "Get:16 https://packages.microsoft.com/ubuntu/22.04/prod jammy/main amd64 Packages [268 kB]\n",
        "Get:17 https://packages.microsoft.com/ubuntu/22.04/prod jammy/main armhf Packages [21.8 kB]\n",
        "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,822 kB]\n",
        "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,411 kB]\n",
        "Get:20 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,963 kB]\n",
        "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,161 kB]\n",
        "Get:22 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,520 kB]\n",
        "Get:23 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,289 kB]\n",
        "Get:24 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,852 kB]\n",
        "Fetched 33.9 MB in 38s (901 kB/s)\n",
        "Reading package lists... Done\n",
        "W: https://packages.microsoft.com/ubuntu/22.04/prod/dists/jammy/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
        "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
        "Reading package lists... Done\n",
        "Building dependency tree... Done\n",
        "Reading state information... Done\n",
        "The following additional packages will be installed:\n",
        "  odbcinst unixodbc\n",
        "The following NEW packages will be installed:\n",
        "  msodbcsql18 odbcinst unixodbc\n",
        "0 upgraded, 3 newly installed, 0 to remove and 47 not upgraded.\n",
        "Need to get 791 kB of archives.\n",
        "After this operation, 164 kB of additional disk space will be used.\n",
        "Get:1 https://packages.microsoft.com/ubuntu/22.04/prod jammy/main amd64 msodbcsql18 amd64 18.5.1.1-1 [755 kB]\n",
        "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 odbcinst amd64 2.3.9-5ubuntu0.1 [9,930 B]\n",
        "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 unixodbc amd64 2.3.9-5ubuntu0.1 [26.7 kB]\n",
        "Fetched 791 kB in 0s (2,083 kB/s)\n",
        "Preconfiguring packages ...\n",
        "Selecting previously unselected package odbcinst.\n",
        "(Reading database ... 125080 files and directories currently installed.)\n",
        "Preparing to unpack .../odbcinst_2.3.9-5ubuntu0.1_amd64.deb ...\n",
        "Unpacking odbcinst (2.3.9-5ubuntu0.1) ...\n",
        "Selecting previously unselected package unixodbc.\n",
        "Preparing to unpack .../unixodbc_2.3.9-5ubuntu0.1_amd64.deb ...\n",
        "Unpacking unixodbc (2.3.9-5ubuntu0.1) ...\n",
        "Selecting previously unselected package msodbcsql18.\n",
        "Preparing to unpack .../msodbcsql18_18.5.1.1-1_amd64.deb ...\n",
        "Unpacking msodbcsql18 (18.5.1.1-1) ...\n",
        "Setting up odbcinst (2.3.9-5ubuntu0.1) ...\n",
        "Setting up unixodbc (2.3.9-5ubuntu0.1) ...\n",
        "Setting up msodbcsql18 (18.5.1.1-1) ...\n",
        "odbcinst: Driver installed. Usage count increased to 1.\n",
        "    Target directory is /etc\n",
        "Processing triggers for man-db (2.10.2-1) ..."
      ],
      "metadata": {
        "id": "-NBJWZ_m-zAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import create_engine\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "# ✅ Correct connection details\n",
        "server   = 'badm-t.database.windows.net'   # SQL Server\n",
        "database = 'badm'                          # SQL Database\n",
        "username = 'admin@123@badm-t'              # Full admin login (from portal)\n",
        "password = 'Badm@123'                      # Your password\n",
        "\n",
        "driver = '{ODBC Driver 18 for SQL Server}'\n",
        "odbc_str = (\n",
        "    f\"DRIVER={driver};SERVER={server};DATABASE={database};\"\n",
        "    f\"UID={username};PWD={password};Encrypt=yes;TrustServerCertificate=no;\"\n",
        "    \"Connection Timeout=30;\"\n",
        ")\n",
        "\n",
        "# Create SQLAlchemy engine\n",
        "engine = create_engine(\n",
        "    f\"mssql+pyodbc:///?odbc_connect={quote_plus(odbc_str)}\",\n",
        "    fast_executemany=True\n",
        ")\n",
        "\n",
        "# Test connection\n",
        "with engine.connect() as conn:\n",
        "    print(\"✅ Connected successfully to Azure SQL Database:\", database)\n"
      ],
      "metadata": {
        "id": "k6tFguxP-0sV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08feabd3-e571-447b-f47a-24bdb1a7e606"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Connected successfully to Azure SQL Database: badm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "✅ Connected successfully to Azure SQL Database: badm"
      ],
      "metadata": {
        "id": "jat44Njg-5_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "odbc_str = (\n",
        "    f\"DRIVER={driver};SERVER={server};DATABASE=master;\"\n",
        "    f\"UID={username};PWD={password};Encrypt=yes;TrustServerCertificate=no;\"\n",
        "    \"Connection Timeout=120;\"\n",
        ")\n",
        "test_engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={quote_plus(odbc_str)}\")\n",
        "with test_engine.connect() as conn:\n",
        "    print(\"✅ Connected to master DB\")\n"
      ],
      "metadata": {
        "id": "k8sJosKk_BvC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4977c3ef-3c13-4bb9-912e-502bb6ee664c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Connected to master DB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "✅ Connected to master DB"
      ],
      "metadata": {
        "id": "4dBqNdAR_ClL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import text\n",
        "\n",
        "commands = [\n",
        "    \"IF OBJECT_ID('dbo.Review', 'U') IS NOT NULL DROP TABLE dbo.Review;\",\n",
        "    \"\"\"\n",
        "    CREATE TABLE dbo.Review (\n",
        "        review_id   VARCHAR(22) PRIMARY KEY,\n",
        "        user_id     VARCHAR(22),\n",
        "        business_id VARCHAR(22),\n",
        "        stars       INT,\n",
        "        useful      INT,\n",
        "        funny       INT,\n",
        "        cool        INT,\n",
        "        text        NVARCHAR(MAX), -- Use NVARCHAR(MAX) for potentially long text\n",
        "        date        DATETIME2      -- Use DATETIME2 for datetime objects\n",
        "    );\n",
        "    \"\"\"\n",
        "]\n",
        "\n",
        "with engine.begin() as conn:\n",
        "    for cmd in commands:\n",
        "        conn.execute(text(cmd))\n",
        "print(\"✅ Azure SQL Review table created successfully!\")"
      ],
      "metadata": {
        "id": "gD5TSiH9_Fty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "✅ Azure SQL Review table created successfully!"
      ],
      "metadata": {
        "id": "xYzxIwkY_Iqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Use the correct file path from the review_filepath variable\n",
        "path = review_filepath\n",
        "chunksize = 100000   # Increased chunk size\n",
        "# You might also consider increasing the command timeout in the engine creation\n",
        "# if timeouts persist with larger chunks.\n",
        "\n",
        "for i, chunk in enumerate(pd.read_json(path, lines=True, chunksize=chunksize)):\n",
        "    # Select only flat columns\n",
        "    flat = chunk[[\n",
        "        \"review_id\", \"user_id\", \"business_id\",\"stars\",\"useful\",\"funny\",\"cool\",\"text\",\"date\"\n",
        "    ]]\n",
        "    try:\n",
        "        flat.to_sql(\"Review\", engine, if_exists=\"append\", index=False)\n",
        "        print(f\"✅ Uploaded chunk {i+1}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error uploading chunk {i+1}: {e}\")\n",
        "        # Optionally, add logic here to retry the upload for this chunk or log the error"
      ],
      "metadata": {
        "id": "ko9qfsLb_KzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "✅ Uploaded chunk 1\n",
        "✅ Uploaded chunk 2\n",
        "✅ Uploaded chunk 3\n",
        "✅ Uploaded chunk 4\n",
        "✅ Uploaded chunk 5\n",
        "✅ Uploaded chunk 6\n",
        "✅ Uploaded chunk 7\n",
        "✅ Uploaded chunk 8\n",
        "✅ Uploaded chunk 9\n",
        "✅ Uploaded chunk 10\n",
        "✅ Uploaded chunk 11\n",
        "✅ Uploaded chunk 12\n",
        "✅ Uploaded chunk 13\n",
        "✅ Uploaded chunk 14\n",
        "✅ Uploaded chunk 15\n",
        "✅ Uploaded chunk 16\n",
        "✅ Uploaded chunk 17\n",
        "✅ Uploaded chunk 18\n",
        "✅ Uploaded chunk 19\n",
        "✅ Uploaded chunk 20\n",
        "✅ Uploaded chunk 21\n",
        "✅ Uploaded chunk 22\n",
        "✅ Uploaded chunk 23\n",
        "✅ Uploaded chunk 24\n",
        "✅ Uploaded chunk 25\n",
        "✅ Uploaded chunk 26\n",
        "✅ Uploaded chunk 27\n",
        "✅ Uploaded chunk 28\n",
        "✅ Uploaded chunk 29\n",
        "✅ Uploaded chunk 30\n",
        "✅ Uploaded chunk 31\n",
        "✅ Uploaded chunk 32\n",
        "✅ Uploaded chunk 33\n",
        "✅ Uploaded chunk 34\n",
        "✅ Uploaded chunk 35\n",
        "✅ Uploaded chunk 36\n",
        "✅ Uploaded chunk 37\n",
        "✅ Uploaded chunk 38\n",
        "✅ Uploaded chunk 39\n",
        "✅ Uploaded chunk 40\n",
        "✅ Uploaded chunk 41\n",
        "✅ Uploaded chunk 42\n",
        "✅ Uploaded chunk 43\n",
        "✅ Uploaded chunk 44\n",
        "✅ Uploaded chunk 45\n",
        "✅ Uploaded chunk 46\n",
        "✅ Uploaded chunk 47\n",
        "✅ Uploaded chunk 48\n",
        "✅ Uploaded chunk 49\n",
        "✅ Uploaded chunk 50\n",
        "✅ Uploaded chunk 51\n",
        "✅ Uploaded chunk 52\n",
        "✅ Uploaded chunk 53\n",
        "✅ Uploaded chunk 54\n",
        "✅ Uploaded chunk 55\n",
        "✅ Uploaded chunk 56\n",
        "✅ Uploaded chunk 57\n",
        "✅ Uploaded chunk 58\n",
        "✅ Uploaded chunk 59\n",
        "✅ Uploaded chunk 60\n",
        "✅ Uploaded chunk 61\n",
        "✅ Uploaded chunk 62\n",
        "✅ Uploaded chunk 63\n",
        "✅ Uploaded chunk 64\n",
        "✅ Uploaded chunk 65\n",
        "✅ Uploaded chunk 66\n",
        "✅ Uploaded chunk 67\n",
        "✅ Uploaded chunk 68\n",
        "✅ Uploaded chunk 69\n",
        "✅ Uploaded chunk 70"
      ],
      "metadata": {
        "id": "vQzL5P8u_N5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import text\n",
        "\n",
        "# You can add the foreign key constraints later using ALTER TABLE statements like these:\n",
        "command = [\n",
        "  \"\"\"\n",
        "  ALTER TABLE dbo.Review\n",
        "  ADD CONSTRAINT FK_Review_business_id\n",
        "  FOREIGN KEY (business_id) REFERENCES dbo.Business(business_id);\n",
        "  \"\"\"\n",
        "]\n",
        "\n",
        "with engine.begin() as conn:\n",
        "    for cmd in command:\n",
        "        conn.execute(text(cmd))\n",
        "print(\"✅ Azure SQL Review table foreign keys created successfully!\")"
      ],
      "metadata": {
        "id": "6wB13D_K_Q53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "✅ Azure SQL Review table foreign keys created successfully!"
      ],
      "metadata": {
        "id": "jiZ0kTxN_TQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import text\n",
        "\n",
        "command = \"\"\"\n",
        "ALTER TABLE dbo.[User]\n",
        "ADD CONSTRAINT PK_User_user_id PRIMARY KEY (user_id);\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    with engine.begin() as conn:\n",
        "        conn.execute(text(command))\n",
        "    print(\"✅ Added primary key to dbo.User table successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error adding primary key to dbo.User table: {e}\")"
      ],
      "metadata": {
        "id": "WgDgdUDU_Xev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "✅ Added primary key to dbo.User table successfully!"
      ],
      "metadata": {
        "id": "9TzAKv1J_Yg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This query reveals which cities have high customer engagement but lower quantities of restaurants. This identifies cities that have high demands for restaurants but lower supply, signifying which cities would be best for us to open a new restaurant in (in terms of demand)"
      ],
      "metadata": {
        "id": "zRHGLes7_jXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import text\n",
        "\n",
        "commands = [\n",
        "    \"IF OBJECT_ID('dbo.Business', 'U') IS NOT NULL DROP TABLE dbo.Business;\",\n",
        "    \"\"\"\n",
        "    CREATE TABLE dbo.Business (\n",
        "        business_id   VARCHAR(22) PRIMARY KEY,\n",
        "        name          NVARCHAR(255),\n",
        "        address       NVARCHAR(255),\n",
        "        city          NVARCHAR(100),\n",
        "        state         NVARCHAR(10),\n",
        "        postal_code   NVARCHAR(20),\n",
        "        latitude      FLOAT,\n",
        "        longitude     FLOAT,\n",
        "        stars         DECIMAL(3,2),\n",
        "        review_count  INT,\n",
        "        is_open       BIT\n",
        "    );\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    CREATE TABLE dbo.BusinessCategory (\n",
        "        business_id VARCHAR(22),\n",
        "        category NVARCHAR(100),\n",
        "        CONSTRAINT FK_BC_Bus FOREIGN KEY (business_id)\n",
        "        REFERENCES dbo.Business(business_id)\n",
        "    );\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    CREATE TABLE dbo.BusinessHours (\n",
        "        business_id VARCHAR(22),\n",
        "        weekday NVARCHAR(15),\n",
        "        open_time NVARCHAR(10),\n",
        "        close_time NVARCHAR(10),\n",
        "        CONSTRAINT FK_BH_Bus FOREIGN KEY (business_id)\n",
        "        REFERENCES dbo.Business(business_id)\n",
        "    );\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    CREATE TABLE dbo.BusinessAttribute (\n",
        "        business_id VARCHAR(22),\n",
        "        attr_key NVARCHAR(255),\n",
        "        attr_value NVARCHAR(4000),\n",
        "        CONSTRAINT FK_BA_Bus FOREIGN KEY (business_id)\n",
        "        REFERENCES dbo.Business(business_id)\n",
        "    );\n",
        "    \"\"\"\n",
        "]\n",
        "\n",
        "with engine.begin() as conn:\n",
        "    for cmd in commands:\n",
        "        conn.execute(text(cmd))\n",
        "print(\"✅ All Azure SQL tables created successfully!\")\n"
      ],
      "metadata": {
        "id": "dxUPcC1G_xtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "✅ All Azure SQL tables created successfully!"
      ],
      "metadata": {
        "id": "SUWmQh7M_y3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "path = \"/content/yelp/yelp_academic_dataset_business.json\"\n",
        "chunksize = 20000   # adjust as needed\n",
        "\n",
        "for i, chunk in enumerate(pd.read_json(path, lines=True, chunksize=chunksize)):\n",
        "    # Select only flat columns\n",
        "    flat = chunk[[\n",
        "        \"business_id\",\"name\",\"address\",\"city\",\"state\",\"postal_code\",\n",
        "        \"latitude\",\"longitude\",\"stars\",\"review_count\",\"is_open\"\n",
        "    ]]\n",
        "    flat.to_sql(\"Business\", engine, if_exists=\"append\", index=False)\n",
        "    print(f\"✅ Uploaded chunk {i+1}\")\n"
      ],
      "metadata": {
        "id": "Fvap3GNI_1OD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "✅ Uploaded chunk 1\n",
        "✅ Uploaded chunk 2\n",
        "✅ Uploaded chunk 3\n",
        "✅ Uploaded chunk 4\n",
        "✅ Uploaded chunk 5\n",
        "✅ Uploaded chunk 6\n",
        "✅ Uploaded chunk 7\n",
        "✅ Uploaded chunk 8"
      ],
      "metadata": {
        "id": "eQS1MHIB_3jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "path = \"/content/yelp/yelp_academic_dataset_business.json\"\n",
        "chunksize = 20000\n",
        "\n",
        "for chunk in tqdm(pd.read_json(path, lines=True, chunksize=chunksize), desc=\"Uploading categories\"):\n",
        "    # keep only needed columns\n",
        "    cat_df = chunk[[\"business_id\", \"categories\"]].dropna(subset=[\"categories\"])\n",
        "    # split comma-separated strings into lists\n",
        "    cat_df[\"categories\"] = cat_df[\"categories\"].str.split(\",\")\n",
        "    # expand list elements into rows (vectorized)\n",
        "    cat_df = cat_df.explode(\"categories\")\n",
        "    # strip whitespace and drop blanks\n",
        "    cat_df[\"categories\"] = cat_df[\"categories\"].str.strip()\n",
        "    cat_df = cat_df[cat_df[\"categories\"].notna() & (cat_df[\"categories\"] != \"\")]\n",
        "    # rename for SQL\n",
        "    cat_df.rename(columns={\"categories\": \"category\"}, inplace=True)\n",
        "\n",
        "    # bulk insert\n",
        "    cat_df.to_sql(\"BusinessCategory\", engine, if_exists=\"append\", index=False)\n",
        "\n",
        "print(\"✅ BusinessCategory table populated successfully!\")\n"
      ],
      "metadata": {
        "id": "tqCIixkr_5Rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Uploading categories: 4it [10:00, 149.79s/it]"
      ],
      "metadata": {
        "id": "k8HKjHgQ_72s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Yelp Academic Dataset (Business file) contains detailed information about businesses listed on Yelp. Each record represents one business and includes identifying information, location coordinates, ratings, reviews, and metadata such as categories, attributes, and operating hours."
      ],
      "metadata": {
        "id": "aBp2p1NLAPZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# The engine you created is already in memory\n",
        "# engine = ...\n",
        "\n",
        "# Define the file path in Colab\n",
        "file_path = 'yelp_academic_dataset_user.json'\n",
        "\n",
        "# These are the simple, one-value-per-user columns\n",
        "simple_user_cols = [\n",
        "    'user_id', 'name', 'review_count', 'yelping_since', 'useful', 'funny',\n",
        "    'cool', 'fans', 'average_stars', 'compliment_hot', 'compliment_more',\n",
        "    'compliment_profile', 'compliment_cute', 'compliment_list', 'compliment_note',\n",
        "    'compliment_plain', 'compliment_cool', 'compliment_funny', 'compliment_writer',\n",
        "    'compliment_photos'\n",
        "]\n",
        "\n",
        "# 1. Keep this chunksize large to read the file efficiently\n",
        "chunk_iter = pd.read_json(file_path, lines=True, chunksize=50000)\n",
        "\n",
        "print(\"Starting to process and upload user.json chunks...\")\n",
        "\n",
        "for i, chunk in enumerate(chunk_iter):\n",
        "    print(f\"  Processing chunk {i}...\")\n",
        "\n",
        "    # --- 1. Load the MAIN 'User' table ---\n",
        "    user_main_chunk = chunk[simple_user_cols]\n",
        "\n",
        "    user_main_chunk.to_sql(\n",
        "        'User',\n",
        "        con=engine,\n",
        "        if_exists='append',\n",
        "        index=False,\n",
        "        method='multi',\n",
        "        chunksize=100  # <-- 2. ADDED THIS LINE: Breaks the 50,000 rows into small DB chunks\n",
        "    )\n",
        "\n",
        "    # --- 2. Load the 'User_Elite' table ---\n",
        "    elite_chunk = chunk[['user_id', 'elite']].dropna(subset=['elite'])\n",
        "\n",
        "    if not elite_chunk.empty:\n",
        "        elite_chunk['elite'] = elite_chunk['elite'].str.split(',')\n",
        "        elite_chunk_exploded = elite_chunk.explode('elite').rename(columns={'elite': 'year'})\n",
        "\n",
        "        elite_chunk_exploded.to_sql(\n",
        "            'User_Elite',\n",
        "            con=engine,\n",
        "            if_exists='append',\n",
        "            index=False,\n",
        "            method='multi',\n",
        "            chunksize=100  # <-- 2. ADDED THIS LINE: Breaks this into small DB chunks\n",
        "        )\n",
        "\n",
        "    # --- 3. SKIPPING User_Friends table ---\n",
        "    # (This is still the right call to save time)\n",
        "\n",
        "print(\"✅ All user.json chunks processed and loaded!\")"
      ],
      "metadata": {
        "id": "QEoJ9UlLAQvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Starting to process and upload user.json chunks...\n",
        "  Processing chunk 0...\n",
        "  Processing chunk 1...\n",
        "  Processing chunk 2...\n",
        "  Processing chunk 3...\n",
        "  Processing chunk 4...\n",
        "  Processing chunk 5...\n",
        "  Processing chunk 6...\n",
        "  Processing chunk 7...\n",
        "  Processing chunk 8...\n",
        "  Processing chunk 9...\n",
        "  Processing chunk 10...\n",
        "  Processing chunk 11...\n",
        "  Processing chunk 12...\n",
        "  Processing chunk 13...\n",
        "  Processing chunk 14...\n",
        "  Processing chunk 15...\n",
        "  Processing chunk 16...\n",
        "  Processing chunk 17...\n",
        "  Processing chunk 18...\n",
        "  Processing chunk 19...\n",
        "  Processing chunk 20...\n",
        "  Processing chunk 21...\n",
        "  Processing chunk 22...\n",
        "  Processing chunk 23...\n",
        "  Processing chunk 24...\n",
        "  Processing chunk 25...\n",
        "  Processing chunk 26...\n",
        "  Processing chunk 27...\n",
        "  Processing chunk 28...\n",
        "  Processing chunk 29...\n",
        "  Processing chunk 30...\n",
        "  Processing chunk 31...\n",
        "  Processing chunk 32...\n",
        "  Processing chunk 33...\n",
        "  Processing chunk 34...\n",
        "  Processing chunk 35...\n",
        "  Processing chunk 36...\n",
        "  Processing chunk 37...\n",
        "  Processing chunk 38...\n",
        "  Processing chunk 39...\n",
        "✅ All user.json chunks processed and loaded!"
      ],
      "metadata": {
        "id": "4MgZpDDsAUjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The user.json file is a 3.3GB line-delimited JSON file. Each line represents a single user profile.\n",
        "\n",
        "Total Rows: ~1.98 million users.\n",
        "\n",
        "Primary Key: user_id is the unique identifier for each user."
      ],
      "metadata": {
        "id": "Ikh26CE7Ae0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import text\n",
        "\n",
        "commands = [\n",
        "    \"IF OBJECT_ID('dbo.Tip', 'U') IS NOT NULL DROP TABLE dbo.Tip;\",\n",
        "    \"\"\"\n",
        "    CREATE TABLE dbo.Tip (\n",
        "        tip_id        INT IDENTITY(1,1) PRIMARY KEY,\n",
        "        user_id       VARCHAR(22) NOT NULL,\n",
        "        business_id   VARCHAR(22) NOT NULL,\n",
        "        text          NVARCHAR(MAX),\n",
        "        date          DATETIME2,\n",
        "        INDEX IX_Tip_UserBusiness (user_id, business_id, date)\n",
        "    );\n",
        "    \"\"\"\n",
        "]\n",
        "\n",
        "with engine.begin() as conn:\n",
        "    for cmd in commands:\n",
        "        conn.execute(text(cmd))\n",
        "print(\"✅ Tip table created successfully!\")"
      ],
      "metadata": {
        "id": "OqG-C3ogAW4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/yelp_academic_dataset_tip.json'\n",
        "chunksize = 20000   # adjust as needed\n",
        "\n",
        "for i, chunk in enumerate(pd.read_json(file_path, lines=True, chunksize=chunksize)):\n",
        "    # Select all columns from tip dataset\n",
        "    flat = chunk[[\"user_id\", \"business_id\", \"text\", \"date\"]]\n",
        "\n",
        "    # Convert date string to datetime\n",
        "    flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "\n",
        "    flat.to_sql(\"Tip\", engine, if_exists=\"append\", index=False)\n",
        "    print(f\"✅ Uploaded chunk {i+1}\")"
      ],
      "metadata": {
        "id": "Zk_LdzXlAtLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 1\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 2\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 3\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 4\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 5\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 6\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 7\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 8\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 9\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 10\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 11\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 12\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 13\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 14\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 15\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 16\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 17\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 18\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 19\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 20\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 21\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 22\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 23\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 24\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 25\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 26\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 27\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 28\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 29\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 30\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 31\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 32\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 33\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 34\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 35\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 36\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 37\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 38\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 39\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 40\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 41\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 42\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 43\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 44\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 45\n",
        "/tmp/ipython-input-945981860.py:11: SettingWithCopyWarning:\n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  flat[\"date\"] = pd.to_datetime(flat[\"date\"])\n",
        "✅ Uploaded chunk 46"
      ],
      "metadata": {
        "id": "lbw1pogXA2kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "file_path = '/content/yelp_academic_dataset_tip.json'\n",
        "chunksize = 20000\n",
        "\n",
        "for chunk in tqdm(pd.read_json(file_path, lines=True, chunksize=chunksize), desc=\"Uploading tips\"):\n",
        "    # Keep only needed columns\n",
        "    tip_df = chunk[[\"user_id\", \"business_id\", \"text\", \"date\"]].copy()\n",
        "\n",
        "    # Convert date string to datetime\n",
        "    tip_df[\"date\"] = pd.to_datetime(tip_df[\"date\"])\n",
        "\n",
        "    # Drop any rows with missing required fields\n",
        "    tip_df = tip_df.dropna(subset=[\"user_id\", \"business_id\", \"date\"])\n",
        "\n",
        "    # Bulk insert\n",
        "    tip_df.to_sql(\"Tip\", engine, if_exists=\"append\", index=False)\n",
        "\n",
        "print(\"✅ Tip table populated successfully!\")"
      ],
      "metadata": {
        "id": "hXCsxKdkA71n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Uploading tips: 46it [01:40,  2.18s/it]✅ Tip table populated successfully!\n"
      ],
      "metadata": {
        "id": "KjqEeh9GBTlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/kaggle/input/yelp-dataset/\")\n",
        "\n",
        "print(os.getcwd())\n",
        "\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data_file = open(\"yelp_academic_dataset_checkin.json\")\n",
        "\n",
        "data = []\n",
        "\n",
        "for line in data_file:\n",
        "\n",
        "    data.append(json.loads(line))\n",
        "\n",
        "checkin_df = pd.DataFrame(data)\n",
        "\n",
        "data_file.close()\n"
      ],
      "metadata": {
        "id": "BEowYdYJBWLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the 'date' column into individual rows\n",
        "checkin_df['date'] = checkin_df['date'].str.split(', ')\n",
        "checkin_df = checkin_df.explode('date').reset_index(drop=True)\n",
        "\n",
        "# Add a 'checkin_ID' column as the primary key\n",
        "checkin_df['checkin_ID'] = checkin_df.index + 1\n",
        "\n",
        "# Reorder columns to put 'checkin_ID' first\n",
        "checkin_df = checkin_df[['checkin_ID', 'business_id', 'date']]\n",
        "\n",
        "\n",
        "# Display the updated dataframe\n",
        "display(checkin_df.head())"
      ],
      "metadata": {
        "id": "JtIuxrlrE0Zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "checkin_ID\tbusiness_id\tdate\n",
        "0\t1\t---kPU91CF4Lq2-WlRu9Lw\t2020-03-13 21:10:56\n",
        "1\t2\t---kPU91CF4Lq2-WlRu9Lw\t2020-06-02 22:18:06\n",
        "2\t3\t---kPU91CF4Lq2-WlRu9Lw\t2020-07-24 22:42:27\n",
        "3\t4\t---kPU91CF4Lq2-WlRu9Lw\t2020-10-24 21:36:13\n",
        "4\t5\t---kPU91CF4Lq2-WlRu9Lw\t2020-12-09 21:23:33\n"
      ],
      "metadata": {
        "id": "thM3LPCVE3jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import text\n",
        "\n",
        "commands = [\n",
        "    \"IF OBJECT_ID('dbo.Checkin', 'U') IS NOT NULL DROP TABLE dbo.Checkin;\",\n",
        "    \"\"\"\n",
        "    CREATE TABLE dbo.Checkin (\n",
        "        checkin_ID INT PRIMARY KEY,\n",
        "        business_id VARCHAR(22),\n",
        "        date DATETIME2,\n",
        "        CONSTRAINT FK_Checkin_Bus FOREIGN KEY (business_id)\n",
        "        REFERENCES dbo.Business(business_id)\n",
        "    );\n",
        "    \"\"\"\n",
        "]\n",
        "\n",
        "with engine.begin() as conn:\n",
        "    for cmd in commands:\n",
        "        conn.execute(text(cmd))\n",
        "print(\"✅ All Azure SQL tables created successfully!\")"
      ],
      "metadata": {
        "id": "ZDHtqbZGFCEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "✅ All Azure SQL tables created successfully!"
      ],
      "metadata": {
        "id": "1yjas6PuFFav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import runtime_checkable\n",
        "from typing import runtime_checkable\n",
        "# Helper function to run and preview results safely\n",
        "def run_query(title, sql, preview=5):\n",
        "    print(f\"\\n🔹 {title}\")\n",
        "    with engine.connect() as conn:\n",
        "        result = conn.execute(text(sql)).fetchmany(preview)\n",
        "        for row in result:\n",
        "            print(row)\n",
        "    print(f\"✅ {title} complete\\n{'-'*80}\")\n",
        "\n",
        "run_query(\"TEST\",\n",
        "          \"\"\"\n",
        "          SELECT category, COUNT(*) AS category_count\n",
        "          FROM BusinessCategory\n",
        "          GROUP BY category\n",
        "          ORDER BY category_count DESC;\n",
        "          \"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLNq42JUZvgF",
        "outputId": "ba4c8bac-67b3-4540-9a2d-ead3afb20a8c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 TEST\n",
            "('Restaurants', 59221)\n",
            "('Food', 31534)\n",
            "('Shopping', 27609)\n",
            "('Home Services', 16263)\n",
            "('Beauty & Spas', 16168)\n",
            "✅ TEST complete\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import text\n",
        "\n",
        "# Helper function to run and preview results safely\n",
        "def run_query(title, sql, preview=5):\n",
        "    print(f\"\\n🔹 {title}\")\n",
        "    with engine.connect() as conn:\n",
        "        result = conn.execute(text(sql)).fetchmany(preview)\n",
        "        for row in result:\n",
        "            print(row)\n",
        "    print(f\"✅ {title} complete\\n{'-'*80}\")\n",
        "\n",
        "# 1️⃣  City-level demand vs. supply (limit for speed)\n",
        "run_query(\n",
        "    \"Query 1 – High-Demand Cities (Review-to-Restaurant Ratio)\",\n",
        "    \"\"\"\n",
        "    SELECT TOP 20\n",
        "        b.city,\n",
        "        COUNT(DISTINCT b.business_id) AS restaurants_per_city,\n",
        "        COUNT(r.review_id) AS total_city_reviews,\n",
        "        CAST(COUNT(r.review_id) AS FLOAT)/COUNT(DISTINCT b.business_id) AS reviews_per_restaurant\n",
        "    FROM dbo.Business b\n",
        "    JOIN dbo.Review r ON b.business_id = r.business_id\n",
        "    WHERE b.city IS NOT NULL\n",
        "    GROUP BY b.city\n",
        "    HAVING COUNT(DISTINCT b.business_id) > 5\n",
        "    ORDER BY reviews_per_restaurant DESC;\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# 2️⃣  Category gap\n",
        "run_query(\n",
        "    \"Query 2 – Category Opportunity Gap (Least Represented)\",\n",
        "    \"\"\"\n",
        "    SELECT TOP 10\n",
        "        category,\n",
        "        COUNT(*) AS category_count\n",
        "    FROM dbo.BusinessCategory\n",
        "    GROUP BY category\n",
        "    ORDER BY category_count DESC;\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# 3️⃣  Neighborhood gaps missing those restaurants\n",
        "run_query(\n",
        "    \"Query 3 – Local Gaps for Italian Restaurants (by Postal Code in CLEARWATER BEACH)\",\n",
        "    \"\"\"\n",
        "    SELECT TOP 10\n",
        "        b.postal_code,\n",
        "        COUNT(DISTINCT b.business_id) AS restaurant_count\n",
        "    FROM dbo.Business b\n",
        "    JOIN dbo.BusinessCategory bc ON b.business_id = bc.business_id\n",
        "    WHERE bc.category LIKE '%Restaurants%'\n",
        "    GROUP BY b.postal_code\n",
        "    ORDER BY restaurant_count ASC;\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# 4️⃣  Top cuisines in Arizona\n",
        "run_query(\n",
        "    \"Query 4 – Top Cuisine Categories in Arizona\",\n",
        "    \"\"\"\n",
        "    SELECT TOP 10 bc.category, COUNT(DISTINCT b.business_id) AS count_restaurants\n",
        "    FROM dbo.Business b\n",
        "    JOIN dbo.BusinessCategory bc ON b.business_id = bc.business_id\n",
        "    WHERE b.state = 'AZ' AND bc.category LIKE '%Restaurant%'\n",
        "    GROUP BY bc.category\n",
        "    ORDER BY count_restaurants DESC;\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# 5️⃣  Cuisine vs rating comparison\n",
        "run_query(\n",
        "    \"Query 5 – Cuisine vs Average Ratings\",\n",
        "    \"\"\"\n",
        "    SELECT bc.category, ROUND(AVG(b.stars),2) AS avg_rating, COUNT(*) AS count_restaurants\n",
        "    FROM dbo.Business b\n",
        "    JOIN dbo.BusinessCategory bc ON b.business_id = bc.business_id\n",
        "    WHERE bc.category IN ('Italian','Mexican','Chinese','Indian','American (Traditional)')\n",
        "    GROUP BY bc.category\n",
        "    ORDER BY avg_rating DESC;\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# 6️⃣  Price range distribution\n",
        "run_query(\n",
        "    \"Query 6 – Price Range Distribution\",\n",
        "    \"\"\"\n",
        "    SELECT TOP 10 attr_value AS price_level, COUNT(*) AS count_businesses\n",
        "    FROM dbo.BusinessAttribute\n",
        "    WHERE attr_key = 'RestaurantsPriceRange2'\n",
        "    GROUP BY attr_value\n",
        "    ORDER BY price_level;\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# 7️⃣  Review sentiment: Italian restaurant success factors\n",
        "run_query(\n",
        "    \"Query 7 – Review Sentiment (Top Italian Restaurants)\",\n",
        "    \"\"\"\n",
        "    SELECT TOP 10 r.business_id, b.name, b.city, r.stars AS review_stars, LEFT(r.text,150) AS sample_text\n",
        "    FROM dbo.Review r\n",
        "    JOIN dbo.Business b ON r.business_id = b.business_id\n",
        "    JOIN dbo.BusinessCategory bc ON b.business_id = bc.business_id\n",
        "    WHERE bc.category = 'Italian' AND b.stars >= 4.0 AND r.stars >= 4\n",
        "    ORDER BY r.stars DESC, b.stars DESC;\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# 8️⃣  Operational insights – Takeout availability\n",
        "run_query(\n",
        "    \"Query 8 – Takeout / Delivery Availability (Italian Restaurants)\",\n",
        "    \"\"\"\n",
        "    SELECT ba.attr_value, COUNT(*) AS restaurant_count\n",
        "    FROM dbo.BusinessAttribute ba\n",
        "    JOIN dbo.BusinessCategory bc ON ba.business_id = bc.business_id\n",
        "    WHERE bc.category = 'Italian' AND ba.attr_key = 'RestaurantsTakeOut'\n",
        "    GROUP BY ba.attr_value;\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# 9️⃣  Seasonal restaurant activity (Checkins)\n",
        "run_query(\n",
        "    \"Query 9 – Seasonal Activity (Monthly Check-ins in Arizona)\",\n",
        "    \"\"\"\n",
        "    SELECT TOP 12 DATENAME(MONTH, c.date) AS month, COUNT(c.checkin_id) AS total_checkins\n",
        "    FROM dbo.Checkin c\n",
        "    JOIN dbo.Business b ON c.business_id = b.business_id\n",
        "    WHERE b.state = 'AZ'\n",
        "    GROUP BY DATENAME(MONTH, c.date)\n",
        "    ORDER BY total_checkins DESC;\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# 🔟  City with highest average rating\n",
        "run_query(\n",
        "    \"Query 10 – Cities with Highest Average Ratings\",\n",
        "    \"\"\"\n",
        "    SELECT TOP 10 b.state, b.city, ROUND(AVG(b.stars),2) AS avg_stars\n",
        "    FROM dbo.Business b\n",
        "    GROUP BY b.state, b.city\n",
        "    HAVING COUNT(*) > 20\n",
        "    ORDER BY avg_stars DESC;\n",
        "    \"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "Igv0mRpYNqIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "429f3ca5-9b77-4f58-868a-d758edacd1c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 Query 1 – High-Demand Cities (Review-to-Restaurant Ratio)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "🔹 Query 1 – High-Demand Cities (Review-to-Restaurant Ratio)\n",
        "('St Pete beach', 75, 10084, 134.45333333333335)\n",
        "('CLEARWATER BEACH', 164, 21476, 130.9512195121951)\n",
        "('Indian Rocks Beach', 84, 10289, 122.48809523809524)\n",
        "('Manayunk', 11, 1335, 121.36363636363636)\n",
        "('St. Pete Beach', 168, 17849, 106.24404761904762)\n",
        " Query 1 – High-Demand Cities (Review-to-Restaurant Ratio) complete\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "🔹 Query 2 – Category Opportunity Gap in Tucson\n",
        "Query 2 – Category Opportunity Gap in Tucson complete\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "🔹 Query 3 – Local Gaps for Italian Restaurants (by Postal Code in Tucson)\n",
        "('85641', 1)\n",
        "('85707', 1)\n",
        "('85737', 1)\n",
        "('85739', 1)\n",
        "('85743', 1)\n",
        "Query 3 – Local Gaps for Italian Restaurants (by Postal Code in Tucson) complete\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "🔹 Query 4 – Top Cuisine Categories in Arizona\n",
        "('Restaurants', 2671)\n",
        "('Restaurant Supplies', 4)\n",
        "Query 4 – Top Cuisine Categories in Arizona complete\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "🔹 Query 5 – Cuisine vs Average Ratings\n",
        "('Indian', Decimal('3.840000'), 959)\n",
        "('Mexican', Decimal('3.520000'), 5239)\n",
        "('Italian', Decimal('3.510000'), 5207)\n",
        "('American (Traditional)', Decimal('3.400000'), 9173)\n",
        "('Chinese', Decimal('3.390000'), 3580)\n",
        "Query 5 – Cuisine vs Average Ratings complete\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "🔹 Query 6 – Price Range Distribution\n",
        "('1', 3506)\n",
        "('2', 5952)\n",
        "('3', 802)\n",
        "('4', 149)\n",
        "('None', 8)\n",
        " Query 6 – Price Range Distribution complete\n",
        "--------------------------------------------------------------------------------\n",
        "🔹 Query 8 – Takeout / Delivery Availability (Italian Restaurants)\n",
        "('RestaurantsDelivery', 'False', 182)\n",
        "('RestaurantsTakeOut', 'False', 40)\n",
        "('RestaurantsDelivery', 'None', 38)\n",
        "('RestaurantsTakeOut', 'None', 26)\n",
        "('RestaurantsDelivery', 'True', 280)\n",
        "Query 8 – Takeout / Delivery Availability (Italian Restaurants) complete\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "🔹 Query 9 – Seasonal Activity (Monthly Check-ins in Arizona)\n",
        "('March', 73279)\n",
        "('May', 70035)\n",
        "('August', 69887)\n",
        "('April', 69102)\n",
        "('October', 68702)\n",
        "Query 9 – Seasonal Activity (Monthly Check-ins in Arizona) complete\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "🔹 Query 10 – Cities with Highest Average Ratings\n",
        "('NV', 'Virginia City', Decimal('4.330000'))\n",
        "('CA', 'Montecito', Decimal('4.160000'))\n",
        "('PA', 'Washington Crossing', Decimal('4.140000'))\n",
        "('PA', 'Wyndmoor', Decimal('4.130000'))\n",
        "('FL', 'Safety Harbor', Decimal('4.130000'))\n",
        "Query 10 – Cities with Highest Average Ratings complete\n",
        "--------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "vIEsLrRKN0zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight\n",
        "\n",
        "1\tCity demand vs. supply\tReveals where restaurant demand exceeds supply (potential launch cities).\n",
        "\n",
        "2\tCategory gaps\tFinds cuisines underrepresented but highly rated in that city.\n",
        "\n",
        "3\tNeighborhood gaps\tPinpoints neighborhoods lacking that cuisine.\n",
        "\n",
        "4\tTop cuisines\tShows state-level popularity for contextual benchmarking.\n",
        "\n",
        "5\tCuisine vs rating\tCompares perception of major cuisines to choose quality benchmarks.\n",
        "\n",
        "6\tPrice range\tGuides pricing strategy (e.g., mid-tier most common).\n",
        "\n",
        "7\tReview sentiment\tIdentifies success factors from positive reviews (authenticity, service).\n",
        "\n",
        "8\tOperational features\tHighlights gaps in delivery/takeout availability.\n",
        "\n",
        "9\tSeasonal demand\tReveals peak seasons for marketing/staffing.\n",
        "\n",
        "10\tTop-rated cities\tValidates geographic opportunities for expansion."
      ],
      "metadata": {
        "id": "uV7LO94cQ8n-"
      }
    }
  ]
}
